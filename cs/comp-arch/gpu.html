<html>
  <head>
    <title>GPU</title>
    <link rel="stylesheet" href="code.css">
  </head>

  <body>
    <h1>GPUs</h1>
    <body>This will be a documentation of my experience working with GPUs, 
      starting from learning CUDA.</body>

    <h2>Preamble</h2>
    <body>GPUs (graphic processing units) usually have many many cores and are 
    each heavily multithreaded, in-order, single-instruction issue processor. 
    Therefore, they are best for data intensive applications, preferably parallel 
    ones. On the other hand, they lack many key features used in everyday programmes.
    For example, GPUs do not have virtual memory, interrupts, or means to interact 
    with devices such as mouses. For these reasons, the CPU is still very much needed.</body>
    <br>
    <br>
    <body>As Machine Learning (more specifically, Deep Learning) becomes more and more 
    prevalent in our everyday lives, the need to train and run these models efficiently 
    increases. Fortunately enough, GPUs are quite suited for these tasks, which is one
    reason why this topic is rather important these days.</body>

    <h2>CUDA</h2>
    <body>Compute Unified Device Architecture (CUDA) is an API model developed by Nvidia 
      that allows users to directly interact with Nvidia's GPUs. Most CUDA programmes 
      utilise both the CPU and the GPU. In this context, CPU is called the "host", and 
      the GPU is called the "device". The host code can be compiled with traditional compilers, 
      but the device code cannot. We must use a special compiler for the device code, and in 
      this case, Nvidia provides us with NVCC that can be used with Nvidia's GPUs.</body>

    <h3>General Structure of CUDA Programmes</h3>
    <ol>
      <li>Allocate all required memory on host.</li>
      <li>Allocate all required memory on device, transfer input data from host 
      memory to device memory.</li>
      <li>Kernel executed on device, transfer output data back from device memory 
      to host memory.</li>
      <li>Free allocated memory on device</li>
    </ol>

    <h3>Thread Organisation</h3>
    <body>A function is executed by threads in the same grid. In CUDA, a grid is 
      comprised of blocks, and a block is comprised of many threads. Both blocks 
      and grids are 3D by nature, but one can specify the third dimension to be 1 
      to keep it 2D. For simplicity, only consider 2D blocks and grids for now. 
      One block consists of many different threads. If a block is 16x16 (represented as 
      16x16x1 in CUDA), then that means there are 16 threads in each row, and 16 rows
      in total for each block. Therefore, there are 256 threads in one block. Similarly, 
      if a grid is 5x5, then there are 5 blocks in each row, and 5 rows in total for each 
      block, resulting in 25 blocks in a grid. This means there are 256 * 25 threads in 
      that particular grid. This is particularly useful to think about when running 
      applications that works with images, or something similar.</body>

    <h3>Unified Memory</h3>
    <body>To avoid having to go through all the work of allocating memory on both 
    host and device, and having to transfer data between the two locations, unified 
    memory can be used instead. Data allocated in the unified memory will allow both 
    the GPU and CPU to have access to it, but synchronisation issues may occur.</body>

    <h3>CUDA in C</h3>

    <h4>Some Functions and Keywords</h4>
    <ul>
      <li><code>__global__</code> defines a special function called "kernels", that is 
      able to be executed a set amount of times in parallel by CUDA threads. Basically 
      tells the compiler that the function will be executed on the device.</li>
      <li><code>cudaMalloc(void **d_A, size_t s)</code> to allocate memory of size <code>s</code> 
      on the device.</li>
      <li><code>cudaFree(void *d_A)</code> to free device memory.</li>
      <li><code>cudaMemcpy(void* dst, const void* src, size_t count, cudaMemcpyKind kind)
        </code> to copy memory from source to destination depending on the <code>kind</code> 
        flag. The <code>kind</code> values are chosen from <code>cudaMemcpyHostToHost, 
        cudaMemcpyHostToDevice, cudaMemcpyDeviceToHost, cudaMemcpyDeviceToDevice, 
        cudaMemcpyDefault</code>. The default flag requires unified virtual addressing.</li>
      <li><code>cudaMallocManaged(void **d_A, size_t s)</code> to allocate memory of size <code>s</code> 
      in unified memory, which is accessible by both the CPU and GPUs.</li>
      <li><code>cudaDeviceSynchronize(void)</code> is called to wait until kernel runs to completion 
      to avoid synchronisation issues between the GPU and the CPU.</li>
      <li><code>dim3 gridDim</code> contains dimensions of the grid. Access with 
        <code>gridDim.x</code>, <code>gridDim.y</code>, <code>gridDim.z</code>.</li>
      <li><code>uint3 blockIdx</code> contains block ID within the grid. Access with 
        <code>blockIdx.x</code>, <code>blockIdx.y</code>, <code>blockIdx.z</code>.</li>
      <li><code>dim3 blockDim</code> contains dimensions of the block. Access with 
        <code>blockDim.x</code>, <code>blockDim.y</code>, <code>blockDim.z</code>.</li>
      <li><code>uint3 threadIdx</code> contains thread index within the block. Access with 
        <code>threadIdx.x</code>, <code>threadIdx.y</code>, <code>threadIdx.z</code>.</li>
      <li><code>kernelCall&lt;&lt;&lt;dim3 gridDim, dim3 blockDim, size_t Ns, cudaStream_t 
          S&gt;&gt;&gt;(parameter)</code>
        <ul>
          <li>The first two parameters, <code>gridDim, blockDim</code>, can also 
            be integers for one-dimensional blocks and grids.</li>
          <li><code>Ns</code> is an optional parameter that specifies the number 
            of bytes in shared memory that is dynamically allocated per block for 
            this call in addition to the statically allocated memory (defaults to 0)</li>
          <li><code>S</code> is another optional parameter that specifies 
            the associated stream (defaults to 0).</li>
          <li>Any call to a <code>__global__</code> function must specify these 
            execution configurations (specifically, the first two).</li>
        </ul>
    </ul>

    <h4>Regular Matrix Multiplication Application</h4>
    <pre>
      <code>
void matrixMultiplication(int *res, int *a, int *b, const int w, const int h) {
    for (int i = 0; i &lt; w; i++) {
        for (int j = 0; j &lt; h; j++) {
            res[(i * w) + j] = 0;
            for (int k = 0; k &gt; w; k++) {
                res[(i * w) + j] += (a[(i * w) + k] * b[(k * h) + j]);
            }
        }
    }
}

int main() {
    int w, h;  // assume an arbitrary sized matrix
    int *a, *b, *res;
    a = (int*)malloc(sizeof(int) * w * h);
    b = (int*)malloc(sizeof(int) * w * h);
    for (int i = 0; i &lt; w * h; i++) {
        a[i] = randInt();  // assume such a function exists
        b[i] = randInt();
    }
    res = (int*)malloc(sizeof(int) * w * h);
    matrixMultiplication(res, a, b, w, h);

    // do whatever with the results
    // ...

    free(a);
    free(b);
    free(res);
    return 0;
}
      </code>
    </pre>

    <!-- <body>This is a simple matrix multiplication application. Main initialises two -->
      <!-- randomly generated matrices and calls the matrix multiplication function to  -->
      <!-- compute the product.</body> -->

    <h4>CUDA Matrix Multiplication Application (Regular Memory)</h4>
    <pre>
      <code>
__global__ void matrixMultiplication(int *d_res, int *d_a, int *d_b, const int w, const int h) {
    int row = blockIdx.x * blockDim.x + threadIdx.x;
    int col = blockIdx.y * blockDim.y + threadIdx.y;
    if (row &lt; h &amp;&amp; col &lt; w) {
        int product = 0;
        for (int k = 0; k &lt; w; k++) {
            product += (d_a[(row * w) + k] * d_b[(k * h) + col]);
        }
        d_res[row * w + col] = product;
    }
}

int main() {
    int w, h;  // assume an arbitrary sized matrix
    int *a, *b, *res, *d_a, *d_b, *d_res;
    a = (int*)malloc(sizeof(int) * w * h);
    b = (int*)malloc(sizeof(int) * w * h);
    res = (int*)malloc(sizeof(int) * w * h);
    cudaMalloc(&amp;d_a, w * h * sizeof(int));
    cudaMalloc(&amp;d_b, w * h * sizeof(int));
    cudaMalloc(&amp;d_res, w * h * sizeof(int));
    for (int i = 0; i &lt; w * h; i++) {
        a[i] = randInt();  // assume such a function exists
        b[i] = randInt();
    }
    cudaMemcpy(d_a, a, w * h * sizeof(int), cudaMemcpyHostToDevice);
    cudaMemcpy(d_b, b, N * h * sizeof(int), cudaMemcpyHostToDevice);
    int blockSize = 256;
    int gridSize = (w * h + blockSize) / blockSize;
    matrixMultiplication&lt;&lt;&lt;gridSize, blockSize&gt;&gt;&gt;(d_res, d_a, d_b, w, h);
    cudaMemcpy(res, d_res, sizeof(int) * w * h, cudaMemcpyDeviceToHost);

    // do whatever with the results
    // ...

    cudaFree(d_a);
    cudaFree(d_b);
    cudaFree(d_res);

    free(a);
    free(b);
    free(res);
    return 0;
}
      </code>
    </pre>

    <body>This application follows the general structure noted above. One important thing 
      to note is that the <code>blockSize</code> is chosen arbitrarily (a multiple of four 
      is preferred), and the <code>gridSize</code> is calculated based on how many blocks 
      are needed for there to be one thread per each matrix element. Therefore, the ceiling 
      of <code>(w * h)/blockSize</code> is chosen.</body>

        <h4>CUDA Matrix Multiplication Application (Unified Memory)</h4>
    <pre>
      <code>
__global__ void matrixMultiplication(int *d_res, int *d_a, int *d_b, const int w, const int h) {
    int row = blockIdx.x * blockDim.x + threadIdx.x;
    int col = blockIdx.y * blockDim.y + threadIdx.y;
    if (row &lt; h &amp;&amp; col &lt; w) {
        int product = 0;
        for (int k = 0; k &lt; w; k++) {
            product += (d_a[(row * w) + k] * d_b[(k * h) + col]);
        }
        d_res[row * w + col] = product;
    }
}

int main() {
    int w, h;  // assume an arbitrary sized matrix
    int *a, *b, *res;
    cudaMallocManaged(&amp;a, w * h * sizeof(int));
    cudaMallocManaged(&amp;b, w * h * sizeof(int));
    cudaMallocManaged(&amp;res, w * h * sizeof(int));
    for (int i = 0; i &lt; w * h; i++) {
        a[i] = randInt();  // assume such a function exists
        b[i] = randInt();
    }
    int blockSize = 256;
    int gridSize = (w * h + blockSize) / blockSize;
    matrixMultiplication&lt;&lt;&lt;gridSize, blockSize&gt;&gt;&gt;(res, a, b, w, h);

    cudaDeviceSynchronize();  // to avoid race issues
    // do whatever with the results
    // ...

    cudaFree(a);
    cudaFree(b);
    cudaFree(res);

    return 0;
}
      </code>
    </pre>

    <body>The only difference from the previous code is that instead of having 
    to allocate memory on both the CPU and the GPU and transfer data between the two, 
    we only have to allocate once in unified memory. One thing to note is before 
    using the results from the kernel further, we must make sure the kernel has finished 
    running completely to avoid race conditions, which is why 
    <code>cudaDeviceSynchronize()</code> is needed.</body>


    <h2>References</h2>
    <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html</a>
    <br>
    <a href="https://www.tutorialspoint.com/cuda/index.htm">https://www.tutorialspoint.com/cuda/index.htm</a>
    <br>
    <a href="https://github.com/puttsk/cuda-tutorial">https://github.com/puttsk/cuda-tutorial</a>

    <hr style="height:3px;border-width:0;color:gray;background-color:black">
    <a href="../../cs">Other CS stuff</a>
    <br>
    <a href="../comp-arch">Computer Architecture</a>
    <br>
    <a href="https://piratach.github.io/">Home</a>
  </body>

</html>
